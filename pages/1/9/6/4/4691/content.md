
# Entropy
* table of contents
{: toc}

## Idea

Entropy is a measure of disorder, given by the amount of [[information]] necessary to precisely specify the [[state]] of a system.


## Mathematical definitions

We can give a precise [[mathematics|mathematical]] definition of the entropy in [[probability theory]].


### Entropy measure of a probability

This is just a preliminary definition.

The __entropy measure__ of a probability $p$ (which is a [[real number]] in the [[unit interval]]) is
$$ h(p) \coloneqq - p \log p $$
(or more precisely $h(p) \coloneqq \lim_{q \searrow p} (- q \log q)$, so that the expression is defined when $p = 0$).  Notice that, despite the minus sign, entropy is positive (since $\log p \leq 0$ for $p \leq 1$).

This entropy is $0$ when $p$ is $0$ or $1$; the idea is that either $0$ or $1$ represents maximum information and minimum uncertainty.  The maximum value (minimum information, maximum uncertainty) is $\frac{1}{2} \log 2$, which occurs when $p = 1/2$.

We have not specified the base of the [[logarithm]], which amounts to a constant factor, which we think of as specifying the [[unit of measurement]] of entropy.  Common choices are $2$ (whose unit is the bit), $\mathrm{e}$ (whose unit is the nat), $10$ (whose unit is the bel), and $\root{10}{10}$ (whose unit is the decibel).


### Entropy of a $\sigma$-algebra with respect to a probability measure

This is the general mathematical definition of entropy.

Given a probability [[measure space]] $(X,\mu)$ and a $\sigma$-[[sigma-algebra|algebra]] $\mathcal{M}$ of [[measurable sets]] in $X$, the __entropy__ of $\mathcal{M}$ with respect to $\mu$ is
\[ \label{general} H_\mu(\mathcal{M}) \coloneqq \sup \{ \sum_{A \in \mathcal{F}} h(\mu(A)) \;|\; \mathcal{F} \subseteq \mathcal{M},\; {|\mathcal{F}|} \lt \aleph_0,\; X = \biguplus \mathcal{F} \} .\]

In words, we take a [[supremum]], over all ways of expressing $X$ as an internal [[disjoint union]] of [[finite set|finitely many]] measurable sets in the $\sigma$-algebra $\mathcal{M}$, of the sum, over these measurable sets, of the entropy measures of the measures of these sets.

(Without loss of generality, we do not need the elements of $\mathcal{F}$ to be [[disjoint sets|disjoint]], as long as their intersections are [[null sets]].  Similarly, we do not need their [[union]] to be all of $X$, as long as their union is a [[full set]].  In [[constructive mathematics]] we *must* weaken the latter condition in this way.)

This definition is very general, and it instructive to look at special cases.


### Entropy of the $\sigma$-algebra of all measurable sets

Given a probability space $(X,\mu)$, the __entropy__ of this probability space is entropy, with respect to $\mu$, of the $\sigma$-algebra of *all* measurable subsets of $X$.


### Entropy of a partition

Recall that a __partition__ of a set $X$ is a [[family of subsets]] of $X$ such that $X$ is the [[union]] of the family and any two distinct elements of the family are [[disjoint sets]].  That is, the supremum in the formula above is taken over finite partitions of $X$ into elements of $\mathcal{M}$.

Every partition of a measure space $X$ into measurable sets (indeed, any family of measurable subsets of $X$) generates a $\sigma$-algebra of measurable sets on $X$.  The __entropy__ of a measurable partition $\mathcal{P}$ of a probability measure space $(X,\mu)$ is defined to be the entropy, with respect to $\mu$, of the $\sigma$-algebra generated by $\mathcal{P}$.  The formula (eq:general) may then be written
\[ \label{partition} H_\mu(\mathcal{P}) = \sum_{A \in \mathcal{P}} h(\mu(A)) = - \sum_{A \in \mathcal{P}} \mu(A) \log \mu(A) ,\]
since an infinite sum (of positive terms) may also be defined as a supremum.  (Actually, the supremum in the infinite sum does not quite match the supremum in (eq:general), so there is a bit of a theorem to prove here.)

In most of the following special cases, we will consider only partitions, although it would be possible also to consider more general $\sigma$-algebras.


### Entropy of (a partition of) a discrete probability space

Recall that a __discrete probability space__ is a [[set]] $X$ equipped with a function $\mu\colon X \to [0,1]$ such that $\sum_{i \in X} \mu(i) = 1$.  We make $X$ into a measure space (with every subset measurable) by defining $\mu(A) \coloneqq \sum_{i \in A} \mu(i)$.  Since every set is measurable, any partition of $X$ is a partition into measurable sets.

Given a discrete probability space $(X,\mu)$ and a partition $\mathcal{P}$ of $X$, the __entropy__ of $\mathcal{P}$ with respect to $\mu$ is defined to be the entropy of $\mathcal{P}$ with respect to the probability measure induced by $\mu$.  Simplifying (eq:partition), we find
$$ H_\mu(\mathcal{P}) = \sum_{A \in \mathcal{P}} h(\sum_{i \in A} \mu(i)) = - \sum_{A \in \mathcal{P}} (\sum_{i \in A} \mu(i)) \log (\sum_{i \in A} \mu(i)) .$$

More specially, the __entropy__ of the discrete probability space $(X,\mu)$ is the entropy of the partition of $X$ into [[singletons]]; we find
$$ H_\mu(X) = \sum_{i \in X} h(\mu(i)) = - \sum_{i \in X} \mu(i) \log \mu(i) .$$

Yet more specially, the __entropy__ of a [[finite set]] $X$ is the entropy of $X$ equipped with the uniform discrete probability measure; we find
\[ \label{Boltzmann} H_{unif}(X) = - \sum_{i \in X} \frac{1}{|X|} \log \frac{1}{|X|} = \log {|X|} ,\]
which is probably the earliest mathematical formula for entropy, due to Boltzmann.


### Entropy with respect to an absolutely continuous probability measure on the real line

Recall that a [[Borel measure]] $\mu$ on an [[interval]] $X$ in the [[real line]] is __[[absolutely continuous measure|absolutely continuous]]__ if $\mu(A) = 0$ whenever $A$ is a [[null set]] with respect to [[Lebesgue measure]].  In this case, we can take the [[Radon-Nikodym derivative]] of $\mu$ with respect to Lebesgue measure, to get an [[integrable function]] $\mu'$, called the __probability distribution function__; we recover $\mu$ by
\[ \label{pdf} \mu(A) = \int_A \mu'(x) \mathrm{d}x ,\]
where the integral is taken with respect to Lebesgue measure.

If $\mathcal{P}$ is a partition of an interval $X$ into [[Borel sets]], then the __entropy__ of $\mathcal{P}$ with respect to an integrable function $f$ is the entropy of $\mathcal{P}$ with respect to the measure induced by $f$ using the integral formula (eq:pdf); we find
$$ H_f(\mathcal{P}) = \sum_{A \in \mathcal{P}} h(\int_A f(x) \mathrm{d}x) = - \sum_{A \in \mathcal{P}} (\int_A f(x) \mathrm{d}x) \log (\int_A f(x) \mathrm{d}x) .$$

On the other hand, the __entropy__ of the probability distribution space $(X,f)$ is the entropy of the entire $\sigma$-algebra of all Borel sets with respect to $f$; we find
$$ H_f(X) = - \int f(x) \log f(x) \mathrm{d}x $$
by a fairly complicated argument.
+-- {: .query}
I haven\'t actually managed to check this argument yet, although I remember it as a true fact.
=--
Note that this $\sigma$-algebra is *not* generated by a partition.


### Entropy of a density matrix

In the analogy between [[classical physics]] and [[quantum physics]], we move from probability distributions on a [[phase space]] to [[density operators]] on a [[Hilbert space]].

So just as the entropy of a probability distribution $f$ on phase space $X$ is given by $- \int f \log f$, so the __entropy__ of a density operator $\rho$ is
$$ H_\rho \coloneqq = - Tr (\rho \log \rho) ,$$
using the [[functional calculus]] on the Hilbert space.

These are both special cases of the entropy of a [[state]] on a $C^*$-[[C-star-algebra|algebra]].

There is a way to fit this into the framework given by (eq:general), but I don\'t remember it (and never really understood it).


## Physical entropy

As hinted above, any probability distribution on a [[phase space]] in [[classical physics]] has an entropy, and any [[density matrix]] on a [[Hilbert space]] in [[quantum physics]] has an entropy.  However, neither of these is the usual entropy in [[physics]].

Instead, physicists use *coarse-grained* entropy, which corresponds mathematically to taking the entropy of a $\sigma$-algebra much smaller than the $\sigma$-algebra of all measurable sets.  Given a classical system with $N$ microscopic degrees of freedom, we identify $n$ macroscopic degrees of freedom that we can reasonably expect to measure, giving a map from $\mathbb{R}^N$ to $\mathbb{R}^n$ (or more generally, a map from an $N$-dimensional microscopic phase space to an $n$-dimensional macroscopic phase space). Then the $\sigma$-algebra of all measurable sets in $\mathbb{R}^n$ [[pullback|pulls back]] to a $\sigma$-algebra on $\mathbb{R}^N$, and the __macroscopic physical entropy__ of a statistical state is the entropy of this $\sigma$-algebra.

Generally, we specify a state by a point in $\mathbb{R}^n$, a macroscopic pure state, and assume a uniform probability distribution on its [[fibre]] in $\mathbb{R}^N$.  If this fibre were a finite set, then we would recover Boltzmann\'s formula (eq:Boltzmann).  This is almost never exactly true in classical statistical physics, but it is often nevertheless a very good approximation.  (Boltzmann\'s formula actually makes better physical sense in quantum statistical physics.)


[[!redirects entropy]]
[[!redirects entropies]]
