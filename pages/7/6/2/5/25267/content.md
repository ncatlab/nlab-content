
#Contents#
* table of contents
{:toc}

## Idea

*Singular learning theory* applies results from [[algebraic geometry]] to [[statistical learning theory]]. In the case of learning algorithms, such as deep neural networks, where there are multiple parameter values corresponding to the same statistical distribution, the [[preimage]] of the target distribution may take the form of a [[singular point|singular]] subspace of the parameter space. Techniques from algebraic geometry may then be applied to study learning with such devices.

## References

Textbook treatments:

* Sumio Watanabe, _Algebraic geometry and statistical learning theory_, CRC Press, 2009.

* Sumio Watanabe, _Mathematical theory of Bayesian statistics_, Cambridge University Press, 2018.

For an informal discussion:

* {#Hoogland} Jesse Hoogland, _Neural networks generalize because of this one weird trick_ ([blog post](https://www.jessehoogland.com/article/neural-networks-generalize-because-of-this-one-weird-trick)); Jesse Hoogland, Filip Sondej, _Spooky action at a distance in the loss landscape_ ([blog post](https://www.lesswrong.com/posts/2N7eEKDuL5sHQou3N/spooky-action-at-a-distance-in-the-loss-landscape)).

For a series of talks and further texts:

* Singular Learning Theory seminar, ([webpage](https://metauni.org/slt/))