
+-- {: .rightHandSide}
+-- {: .toc .clickDown tabindex="0"}
### Context
#### Measure and probability theory
+-- {: .hide}
[[!include measure theory - contents]]
=--
=--
=--

# Entropy
* table of contents
{: toc}

## Idea

Entropy is a measure of disorder, given by the amount of [[information]] necessary to precisely specify the [[state]] of a system.

Entropy is important in [[information theory]] and [[statistical physics]].


## Mathematical definitions

We can give a precise [[mathematics|mathematical]] definition of the entropy in [[probability theory]].


### Expected information of verification

This is just a preliminary definition.

The __expected information of verification__ of a probability (a [[real number]] in the [[unit interval]]) $p$ is
$$ h(p) \coloneqq - p \log p $$
(or more precisely $h(p) \coloneqq \lim_{q \searrow p} (- q \log q)$, so that $h(0) = 0$ is defined).  Notice that, despite the minus sign in this formula, $h$ is a nonnegative function (since $\log p \leq 0$ for $p \leq 1$).  It is also important that $h$ is [[convex function|concave]].

Both $h(0)$ and $h(1)$ are $0$, but for different reasons; $h(1) = 0$ because, upon verifying a statement with probability $1$, one gains no information; while $h(0) = 0$ because one expects never to verify a statement with probability $0$.  In general, $-\log p$ is the information gained by verifying a statement of probability $p$, but this will happen only with probability $p$, hence $-p \log p$.

We have not specified the base of the [[logarithm]], which amounts to a constant factor (proportional to the logarithm of the base), which we think of as specifying the [[unit of measurement]] of entropy.  Common choices for the base are $2$ (whose unit is the [[bit]], originally a unit of memory in computer science), $256$ (byte: $8$ bits), $3$ (trit), $\mathrm{e}$ (nat or neper), $10$ (bel, originally a unit of power intensity in telegraphy, or ban, dit, or hartley), and $\root{10}{10}$ (decibel: $1/10$ of a bel).  In applications to [[statistical physics]], common bases are approximately $10^{3.1456 \times 10^{22}}$ (joule per kelvin), $1.65404$ (calorie per mole-kelvin), etc.


### Entropy of a $\sigma$-algebra on a probability space

This is a general mathematical definition of entropy.

Given a probability [[measure space]] $(X,\mu)$ and a $\sigma$-[[sigma-algebra|algebra]] $\mathcal{M}$ of [[measurable sets]] in $X$, the __entropy__ of $\mathcal{M}$ with respect to $\mu$ is
\[ \label{general} H_\mu(\mathcal{M}) \coloneqq \sup \{ \sum_{A \in \mathcal{F}} h(\mu(A)) \;|\; \mathcal{F} \subseteq \mathcal{M},\; {|\mathcal{F}|} \lt \aleph_0,\; X = \biguplus \mathcal{F} \} .\]

In words, the entropy is the [[supremum]], over all ways of expressing $X$ as an internal [[disjoint union]] of [[finite set|finitely many]] elements of the $\sigma$-algebra $\mathcal{M}$, of the sum, over these measurable sets, of the expected information of verification of these sets.  This supremum can also be expressed as a [[convergence|limit]] as we take $\mathcal{F}$ to be finer and finer, since $h$ is concave and the partitions are [[directed set|directed]].

(Without loss of generality, we do not need the elements of $\mathcal{F}$ to be [[disjoint sets|disjoint]], as long as their intersections are [[null sets]].  Similarly, we do not need their [[union]] to be all of $X$, as long as their union is a [[full set]].  In [[constructive mathematics]], it seems that we *must* weaken the latter condition in this way.)

This definition is very general, and it is instructive to look at special cases.


### Entropy of a probability space

Given a probability space $(X,\mu)$, the __entropy__ of this probability space is the entropy, with respect to $\mu$, of the $\sigma$-algebra of *all* measurable subsets of $X$.


### Entropy of a partition of a probability space

Recall that a __[[partition]]__ of a set $X$ is a [[family of subsets|family]] $\mathcal{P}$ of $X$ such that $X$ is the [[union]] of $\mathcal{P}$ and any two distinct elements of $\mathcal{P}$ are [[disjoint sets|disjoint]].  (That is, the supremum in (eq:general) is taken over finite partitions of $X$ into elements of $\mathcal{M}$.)

Every partition of a measure space $X$ into measurable sets (indeed, any family of measurable subsets of $X$) generates a $\sigma$-algebra of measurable sets.  The __entropy__ of a measurable partition $\mathcal{P}$ of a probability measure space $(X,\mu)$ is the entropy, with respect to $\mu$, of the $\sigma$-algebra generated by $\mathcal{P}$.  The formula (eq:general) may then be written
\[ \label{partition} H_\mu(\mathcal{P}) = \sum_{A \in \mathcal{P}} h(\mu(A)) = - \sum_{A \in \mathcal{P}} \mu(A) \log \mu(A) ,\]
since an infinite sum (of positive terms) may also be defined as a supremum.  (Actually, the supremum in the infinite sum does not quite match the supremum in (eq:general), so there is a bit of a theorem to prove here.)

In most of the following special cases, we will consider only partitions, although it would be possible also to consider more general $\sigma$-algebras.


### Entropy of (a partition of) a discrete probability space

Recall that a __discrete probability space__ is a [[set]] $X$ equipped with a function $\mu\colon X \to [0,1]$ such that $\sum_{i \in X} \mu(i) = 1$; since $\mu(i) \gt 0$ is possible for only countably many $i$, we may assume that $X$ is [[countable set|countable]].  We make $X$ into a measure space (with every subset measurable) by defining $\mu(A) \coloneqq \sum_{i \in A} \mu(i)$.  Since every set is measurable, any partition of $X$ is a partition into measurable sets.

Given a discrete probability space $(X,\mu)$ and a partition $\mathcal{P}$ of $X$, the __entropy__ of $\mathcal{P}$ with respect to $\mu$ is defined to be the entropy of $\mathcal{P}$ with respect to the probability measure induced by $\mu$.  Simplifying (eq:partition), we find
$$ H_\mu(\mathcal{P}) = \sum_{A \in \mathcal{P}} h(\sum_{i \in A} \mu(i)) = - \sum_{A \in \mathcal{P}} (\sum_{i \in A} \mu(i)) \log (\sum_{i \in A} \mu(i)) .$$

More specially, the __entropy__ of the discrete probability space $(X,\mu)$ is the entropy of the partition of $X$ into [[singletons]]; we find
$$ H_\mu(X) = \sum_{i \in X} h(\mu(i)) = - \sum_{i \in X} \mu(i) \log \mu(i) .$$
This is actually a special case of the entropy of a probability space, since the $\sigma$-algebra generated by the singletons is the power set of $X$ (at least when $X$ is countable, and the formulas agree in any case).

Yet more specially, the __entropy__ of a [[finite set]] $X$ is the entropy of $X$ equipped with the uniform discrete probability measure; we find
\[ \label{Boltzmann} H_{unif}(X) = - \sum_{i \in X} \frac{1}{|X|} \log \frac{1}{|X|} = \log {|X|} ,\]
which is probably the earliest mathematical formula for entropy, due to Boltzmann.  (Its physical interpretation appears below.)


### Entropy with respect to an absolutely continuous probability measure on the real line

Recall that a [[Borel measure]] $\mu$ on an [[interval]] $X$ in the [[real line]] is __[[absolutely continuous measure|absolutely continuous]]__ if $\mu(A) = 0$ whenever $A$ is a [[null set]] (with respect to [[Lebesgue measure]]).  In this case, we can take the [[Radon-Nikodym derivative]] of $\mu$ with respect to Lebesgue measure, to get an [[integrable function]] $f$, called the __probability distribution function__; we recover $\mu$ by
\[ \label{pdf} \mu(A) = \int_A f(x) \mathrm{d}x ,\]
where the integral is taken with respect to Lebesgue measure.

If $\mathcal{P}$ is a partition of an interval $X$ into [[Borel sets]], then the __entropy__ of $\mathcal{P}$ with respect to an integrable function $f$ is the entropy of $\mathcal{P}$ with respect to the measure induced by $f$ using the integral formula (eq:pdf); we find
$$ H_f(\mathcal{P}) = \sum_{A \in \mathcal{P}} h(\int_A f(x) \mathrm{d}x) = - \sum_{A \in \mathcal{P}} (\int_A f(x) \mathrm{d}x) \log (\int_A f(x) \mathrm{d}x) .$$

On the other hand, the __entropy__ of the probability distribution space $(X,f)$ is the entropy of the entire $\sigma$-algebra of all Borel sets with respect to $f$; we find
$$ H_f(X) = - \int f(x) \log f(x) \mathrm{d}x $$
by a fairly complicated argument.
+-- {: .query}
I haven\'t actually managed to check this argument yet, although my memory tags it as a true fact.  —Toby
=--
Note that this $\sigma$-algebra is *not* generated by a partition.


### Entropy of a density matrix

In the analogy between [[classical physics]] and [[quantum physics]], we move from probability distributions on a [[phase space]] to [[density operators]] on a [[Hilbert space]].

So just as the entropy of a probability distribution $f$ is given by $- \int f \log f$, so the __entropy__ of a density operator $\rho$ is
$$ H_\rho \coloneqq = - Tr (\rho \log \rho) ,$$
using the [[functional calculus]].

These are both special cases of the entropy of a [[state]] on a $C^*$-[[C-star-algebra|algebra]].

There is a way to fit this into the framework given by (eq:general), but I don\'t remember it (and never really understood it).

### Relative entropy
 {#RelativeEntropy}

For two finite probability distributions $(p_i)$ and $(q_i)$, their **relative entropy** is 

$$
  S(p/q) \coloneqq \sum_{k = 1}^n p_k(log p_k - log q_k)
  \,.
$$


Or alternatively, for $\rho, \phi$ two [[density matrix|density matrices]], their relative entropy is 

$$
  S(\rho/\phi) \coloneqq tr \rho(log \rho - log \phi)
  \,.
$$

There is a generalization of these definitions to [[state]]s on general [[von Neumann algebra]]s, due to ([Araki](#Araki)).

For more on this see _[[relative entropy]]_.


## Physical entropy

As hinted above, any probability distribution on a [[phase space]] in [[classical physics]] has an entropy, and any [[density matrix]] on a [[Hilbert space]] in [[quantum physics]] has an entropy.  However, these are __microscopic entropy__, which is not the usual entropy in [[thermodynamics]] and most other branches of [[physics]].  (In particular, microscopic entropy is conserved, rather than increasing with time.)

Instead, physicists use *coarse-grained* entropy, which corresponds mathematically to taking the entropy of a $\sigma$-algebra much smaller than the $\sigma$-algebra of all measurable sets.  Given a classical system with $N$ microscopic degrees of freedom, we identify $n$ macroscopic degrees of freedom that we can reasonably expect to measure, giving a map from $\mathbb{R}^N$ to $\mathbb{R}^n$ (or more generally, a map from an $N$-dimensional microscopic phase space to an $n$-dimensional macroscopic phase space). Then the $\sigma$-algebra of all measurable sets in $\mathbb{R}^n$ [[pullback|pulls back]] to a $\sigma$-algebra on $\mathbb{R}^N$, and the __macroscopic entropy__ of a statistical state is the entropy of this $\sigma$-algebra.  (Typically, $N$ is on the order of [[Avogadro constant|Avogadro's number]], while $n$ is rarely more than half a dozen, and often as small as $2$.)

Generally, we specify a state by a point in $\mathbb{R}^n$, a macroscopic pure state, and assume a uniform probability distribution on its [[fibre]] in $\mathbb{R}^N$.  If this fibre were a finite set, then we would recover Boltzmann\'s formula (eq:Boltzmann).  This is never exactly true in classical statistical physics, but it is often nevertheless a very good approximation.  (Boltzmann\'s formula actually makes better physical sense in quantum statistical physics, even though Boltzmann himself did not live to see this.)

### Gravitational entropy

* gravitational entropy

  * [[Bekenstein-Hawking entropy]]

  * [[generalized second law of thermodynamics]]


## References
{#References}

[Relative entropy](#RelativeEntropy) of [[state]]s on  [[von Neumann algebra]]s was introduced in 

* [[Huzihiro Araki]], _Relative Entropy of States of von Neumann Algebras_ ([pdf](http://www.google.de/url?sa=t&source=web&cd=5&ved=0CEsQFjAE&url=http%3A%2F%2Fwww.ems-ph.org%2Fjournals%2Fshow_pdf.php%3Fissn%3D0034-5318%26vol%3D11%26iss%3D3%26rank%3D9&rct=j&q=entropy%20cocycle%20von%20Neumann%20algebra&ei=n3jrTYyxOI-c-waJvMnPDw&usg=AFQjCNGuJgVUE7CtGPmb2PZLhOOWt1_JPQ&cad=rja))
 {#Araki}

A characterization of [relative entropy](#RelativeEntropy) on finite-[[dimension]]al [[C-star algebra]]s is given in 

* D. Petz, _Characterization of the relative entropy of states of matrix algebras_ ([pdf](http://www.renyi.hu/~petz/pdf/52.pdf))
 {#Petz}


A survey of entropy in [[operator algebra]]s is in 

* Erling Størmer, _Entropy in operator algebras_ ([pdf](http://www.claymath.org/publications/currentvolumes/connes60/Stormer.pdf))

A large collection of references on quantum entropy is in 

* Christopher Fuchs, _References for Research in Quantum Distinguishability
and State Disturbance_ ([pdf](http://www.perimeterinstitute.ca/personal/cfuchs/BigRef.pdf))

A discussion of entropy with an eye towards the [[presheaf topos]] over the [[site]] of finite [[measure spaces]] is in 

* [[Mikhail Gromov]], _In a Search for a Structure, Part I: On Entropy_ (2012) ([pdf](http://www.ihes.fr/~gromov/PDF/structres-entropy-june-2012.pdf))

[[!redirects entropy]]
[[!redirects entropies]]

