
+-- {: .rightHandSide}
+-- {: .toc .clickDown tabindex="0"}
### Context
#### Deduction and Induction
+-- {: .hide}
[[!include deduction and induction - contents]]
=--
#### Philosophy
+-- {: .hide}
[[!include philosophy - contents]]
=--
=--
=--


# Inductive reasoning
* table of contents
{: toc}

## Idea

Inductive reasoning concerns assessments of how likely a statement is to be true, given observations of its consequences. Unlike with [[deductive reasoning]], where once I derive $Q$ from $P$ and $P \Rightarrow Q$, my learning new facts does not affect the derivation, when I reason inductively, changes to background knowledge frequently cause me to alter my inferences. For example, from observations of ten fish caught randomly from a pond being trout, I may assess the probability that all fish in the pond are trout as $p$. But learning about the number of fish species present in other ponds nearby, may cause me to alter this assessment. It is clear that many factors may have a bearing on our assessment of a degree of belief.

Not everybody holds that inductive reasoning occurs. [[Karl Popper]] famously claimed that "Induction, i.e. inference based on many observations, is a myth. It is neither a psychological fact, nor a fact of ordinary life, nor one of scientific procedure." (Conjectures and Refutations, p. 53). This claim was made primarily about the process of coming to believe in a universal law from the observation of a finite number of instances.

## Mathematical induction
 {#MathematicalInduction}

However, there is no such problem if one can be sure that one is sampling a _complete_ list of instances of a general law. From that one may indeed _induce_ the general law. This is _[[induction|mathematical induction]]_. 

The classical case is induction over the [[natural numbers]]: if one knows a statement about elements of the set $\mathbb{N}$ to hold for the element $0$ and if one knows that if the statement holds for any $n \in \mathbb{N}$, then it also holds for $n+1$, then one can in fact be sure that it holds for each and every $n \in \mathbb{N}$ and the general truth of the statement is induced. 

This is still reflected for instance in the German use of the word: in German _Induktion_ refers to inductive reasoning, but the mathematical induction over the natural numbers is mandatorily called _[vollst√§ndige Induktion](http://de.wikipedia.org/wiki/Vollst%C3%A4ndige_Induktion)_: complete induction.

Notice that, as explained at _[[induction|mathematical induction]]_ the process of induction over the natural numebers is just one special case of a large class of "complete inductions" available in mathematics. In the notion of _[[inductive types]]_ this concept reaches down to the very [[foundations]] of mathematics.

## As Bayesian probability
 {#AsBayesianProbability}

A common approach to formulate an _inductive_ logic is to couch inductive reasoning in terms of [[probability theory]]. It can be shown by so-called "Dutch Book" arguments, that a rational agent must set their degrees of belief in such a way that they satisfy the axioms of probability theory. For example, if your degree of belief that the next toss of a coin will show heads is $p$, then you should believe it will show tails to degree $(1 - p)$, otherwise it will be possible for someone to take betting positions against you making a loss certain whatever the outcome.

Using [[Bayes' Rule]], degrees of belief can be updated on receipt of new evidence.

$$
P(h|e) = P(e|h) \cdot \frac{P(h)}{P(e)},
$$

where $h$ is a hypothesis and $e$ is evidence.

The idea here is that when $e$ is observed, your degree of belief in $h$ should be changed from $P(h)$ to $P(h|e)$. This is known as **conditionalizing**. If $P(h|e) \gt P(h)$, we say that $e$ has provided **confirmation** for $h$.

Typically, situations will involve a range of possible hypotheses, $h_1, h_2, \ldots$, and applying Bayes' Rule will allow us to compare how these fare as new observations are made. For example, comparing the fate of two hypotheses,

$$
\frac{P(h_1|e)}{P(h_2|e)} = \frac{P(e|h_1)}{P(e|h_2)}\cdot \frac{P(h_1)}{P(h_2)}.
$$

How to assign prior probabilities to hypotheses when you don't think you have an exhaustive set of rivals is not obvious. When astronomers in the nineteenth century tried to account for the anomalies in the position of Mercury's perihelion, they tried out all manner of explanations: maybe there was a planet inside Mercury's orbit, maybe there was a cloud of dust surrounding the sun, maybe the power in the inverse square law ought to be (2 - $\epsilon$),... Assigning priors and changing these as evidence comes in is one thing, but it would have been wise to have reserved some of the prior for 'none of the above'.

Interestingly, one of the first people to give a qualitative sketch of how such an approach would work was [[George Polya]] in 'Mathematics and Plausible Reasoning' ([Polya](#Polya)), where examples from mathematics are widely used. The idea of a Bayesian account of plausible reasoning in mathematics surprises many, it being assumed that mathematicians rely solely on [[deductive reasoning|deduction]].

## Related concepts

* [[deductive reasoning]]

## References

* [[Edwin Jaynes]], 2003, _Probability Theory: The Logic of Science_, Cambridge University Press, ([web](http://omega.albany.edu:8008/JaynesBook.html)).

* [[George Polya]], 1954, _Mathematics and Plausible Reasoning, Volume 2:
Patterns of Plausible Inference_, Princeton University Press.
 {#Polya}

Chapter 4 of 

* [[David Corfield]], 2003, _Towards a Philosophy of Real Mathematics_, Cambridge University Press.

From the [[nPOV]], of interest is 

* [[Bob Coecke]], [[Robert Spekkens]]' _Picturing classical and quantum Bayesian inference]_ ([pdf](http://www.cs.ox.ac.uk/people/bob.coecke/PDFS/05-Coecke-Spekkens.pdf)).


[[!redirects inductive reasoning]]
[[!redirects inductive inference]]
[[!redirects inductive logic]]
