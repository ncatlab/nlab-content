# Contents
* table of contents
{: toc}

## Idea

The determinant is the (essentially unique) universal alternating multilinear map.

## Definition

We define the determinant after a few preliminaries 

Let [[Vect]]${}_k$ be the [[category]] of [[vector space|vector spaces]] over a [[field]] $k$, and assume for the moment that the [[characteristic]] $char(k) \neq 2$. For each $j \geq 0$, let 

$$sgn_j \colon S_j \to \hom(k, k)$$ 

be the 1-dimensional [[sign representation]] on the [[symmetric group]] $S_j$, taking each transposition $(i j)$ to $-1 \in k^\ast$. We may linearly extend the sign action of $S_j$, so that $sgn$ names a (right) $k S_j$-[[module]] with underlying [[vector space]] $k$. At the same time, $S_j$ acts on the $j^{th}$ [[tensor product]] of a vector space $V$ by permuting tensor factors, giving a left $k S_j$-module structure on $V^{\otimes j}$. We define the [[Schur functor]] 

$$\Lambda^j \colon Vect_k \to Vect_k$$ 

by the formula 

$$\Lambda^j(V) = sgn_j \otimes_{k S_j} V^{\otimes j}.$$ 

It is called the $j^{th}$ **alternating power** (of $V$). 

+-- {: .num_prop}
###### Proposition 
If $V$ is $n$-[[dimension|dimensional]], then $\Lambda^j(V)$ has dimension $\binom{n}{j}$. In particular, $\Lambda^n(V)$ is 1-dimensional. 
=-- 

+-- {: .proof}
###### Proof 
If $e_1, \ldots, e_n$ is a basis for $V$, then expressions of the form $e_{n_1} \otimes \ldots \otimes e_{n_j}$ form a [[basis of a vector space|basis]] for $V^{\otimes j}$. Let $e_{n_1} \wedge \ldots \wedge e_{n_j}$ denote the [[image]] of this element under the [[quotient]] map $V^{\otimes j} \to \Lambda^j(V)$. We have 

$$e_{n_1} \wedge \ldots \wedge e_{n_i} \wedge e_{n_{i+1}} \wedge \ldots \wedge e_{n_j} = -e_{n_1} \wedge \ldots \wedge e_{n_{i+1}} \wedge e_{n_i} \wedge \ldots \wedge e_{n_j}$$ 

(consider the transposition in $S_j$ which swaps $i$ and $i+1$) and so we may take only such expressions on the left where $n_1 \lt \ldots \lt n_j$ as forming a spanning set for $\Lambda^j(V)$, and indeed these form a basis. The number of such expressions is $\binom{n}{j}$. 
=-- 

+-- {: .num_remark}
###### Remark

In the case where $char(k) = 2$, the same development may be carried out by simply decreeing that $e_{n_1} \wedge \ldots \wedge e_{n_j} = 0$ whenever $n_i = n_{i'}$ for some pair of distinct indices $i$, $i'$. 

=--

Now let $V$ be an $n$-dimensional space, and let $f \colon V \to V$ be a [[linear map]]. By the proposition, the map 

$$\Lambda^n(f) \colon \Lambda^n(V) \to \Lambda^n(V),$$ 

being an [[endomorphism]] on a 1-dimensional space, is given by multiplying by a scalar $D(f) \in k$. It is manifestly [[functor|functorial]] since $\Lambda^n$ is, i.e., $D(f g) = D(f) D(g)$. The quantity $D(f)$ is called the **determinant** of $f$. 

### Determinant of a matrix 

We see then that if $V$ is of dimension $n$, 

$$\det \colon End(V) \to k$$ 

is a [[homomorphism]] of multiplicative [[monoids]]; by commutativity of multiplication in $k$, we infer that 

$$\det(U A U^{-1}) = \det(A)$$ 

for each [[inverse|invertible]] [[linear map]] $U \in GL(V)$. 

If we choose a [[basis of a vector space|basis]] of $V$ so that we have an identification $GL(V) \cong Mat_n(k)$, then the determinant gives a [[function]]

$$\det \colon Mat_n(k) \to k$$ 

that takes products of $n \times n$ [[matrices]] to products in $k$. The determinant however is of course independent of choice of basis, since any two choices are related by a change-of-basis matrix $U$, where $A$ and its transform $U A U^{-1}$ have the same determinant. 

By following the definitions above, we can give an explicit formula: 

$$\det(A) = \sum_{\sigma \in S_n} sgn(\sigma) \prod_{i = 1}^n a_{i \sigma(i)}.$$ 

## Properties

We work over [[fields]] of arbitrary [[characteristic]]. The determinant satisfies the following properties, which taken together uniquely characterize the determinant. Write a square [[matrix]] $A$ as a row of column [[vectors]] $(v_1, \ldots, v_n)$. 

1. $\det$ is separately linear in each column vector: 
$$\det(v_1, \ldots, a v + b w, \ldots, v_n) = a\det(v_1, \ldots, v, \ldots, v_n) + b\det(v_1, \ldots, w, \ldots, v_n)$$ 

1. $\det(v_1, \ldots, v_n) = 0$ whenever $v_i = v_j$ for distinct $i, j$. 

1. $\det(I) = 1$, where $I$ is the identity matrix. 

Other properties may be worked out, starting from the explicit formula or otherwise: 

* If $A$ is a diagonal matrix, then $\det(A)$ is the product of its diagonal entries. 

* More generally, if $A$ is an upper (or lower) triangular matrix, then $\det(A)$ is the product of the diagonal entries. 

* If $E/k$ is an extension field and $f$ is a $k$-linear map $V \to V$, then $\det(f) = \det(E \otimes_k f)$. Using the preceding properties and the [[Jordan normal form]] of a matrix, this means that $\det(f)$ is the product of its eigenvalues (counted with multiplicity), as computed in the algebraic closure of $k$. 

* If $A^t$ is the transpose of $A$, then $\det(A^t) = \det(A)$. 

### Cramer's rule 

A simple observation which flows from these basic properties is 

+-- {: .num_prop} 
###### Proposition
**(Cramer's Rule)**

Let $v_1, \ldots, v_n$ be column vectors of dimension $n$, and suppose 
$$w = \sum_j a_j v_j.$$
Then for each $i$ we have 
$$a_j \det(v_1, \ldots, v_i, \ldots, v_n) = \det(v_1, \ldots, w, \ldots, v_n)$$ 
where $w$ occurs as the $i^{th}$ column vector on the right. 
=-- 

+-- {: .proof}
###### Proof 
This follows straightforwardly from properties 1 and 2 above. 
=-- 

For instance, given a square matrix $A$ such that $\det(A) \neq 0$, and writing $A = (v_1, \ldots, v_n)$, this allows us to solve the equation 

$$A \cdot a = w$$

and we easily conclude that $A$ is invertible if $\det(A) \neq 0$. 

+-- {: .num_remark}
###### Remark

This holds true even if we replace the field $k$ by an arbitrary commutative [[ring]] $R$, and we replace the condition $\det(A) \neq 0$ by the condition that $\det(A)$ is a unit. (The entire development given above goes through, _mutatis mutandis_.) 

=--

### Cayley-Hamilton theorem 

+-- {: .num_lem}
###### Lemma 
Let $R$ be a commutative ring, and let $A$ be an $n \times n$ matrix with entries in $R$. Then there exists an $n \times n$ matrix $\tilde{A}$ with entries in $R$ such that $A \tilde{A} = \tilde{A} A = \det(A) \cdot I_n$. 
=-- 

+-- {: .proof}
###### Proof 
We may as well take $R$ to be the polynomial ring $\mathbb{Z}[a_{i j}]_{1 \leq i, j \leq n}$, since we are then free to interpret the indeterminates $a_{i j}$ however we like along a ring map $\mathbb{Z}[a_{i j}] \to R$. Let $A$ denote the corresponding generic matrix. 

Guided by Cramer's rule, put 

$$\tilde{A}_{j i} = \det(a_1, \ldots, e_i, \ldots a_n),$$ 

the $a_i$ being columns of $A$ and $e_i$, the column vector with $1$ in the $i^{th}$ row and $0$'s elsewhere, appearing as the $j^{th}$ column. If we pretend $A$ is invertible, then we know $A \tilde{A} = \det(A) \cdot I_n = \tilde{A} A$ by Cramer's rule. We claim this holds for general $A$. 

Indeed, we can interpret this as a polynomial equation in $\mathbb{C}[a_{i j}]$ and check it there. As an equation between polynomial functions on the space of matrices $A \in Mat_n(\mathbb{C}) = Spec(\mathbb{C}[a_{i j}])$, it holds on the dense subset $GL_n(\mathbb{C}) \hookrightarrow Mat_n(\mathbb{C})$. Therefore, by continuity, it holds on all of $Mat_n(\mathbb{C})$. But a polynomial function equation with coefficients in $\mathbb{C}$ implies the corresponding polynomial identity, and the proof is complete. 
=-- 


+-- {: .num_thm}
###### Theorem 
(**Cayley-Hamilton**) 
Let $V$ be a finitely generated free module over a commutative ring $R$, and let $f \colon V \to V$ be an $R$-module map. Let $p(t) \in R[t]$ be the **characteristic polynomial** $\det(t \cdot 1_V - f)$ of $f$, and let $\phi_f \colon R[t] \to Mod_R(V, V)$ be the unique $R$-algebra map sending $t$ to $f$. Then $p(f) \coloneqq \phi_f(p)$ is the zero map $0 \colon V \to V$. 

=-- 

+-- {: .proof} 
###### Proof 
Via $\phi_f$, regard $V$ as an $R[t]$-module, and with regard to some $R$-basis $\{v_i\}_{1 \leq i \leq n}$ of $V$, represent $f$ by a matrix $A$. Now consider $t \cdot I_n - A$ as an $n \times n$ matrix $B(t)$ with entries in $R[t]$. By definition of the module structure, this matrix $B(t)$, seen as acting on $V^n$, annihilates the length $n$ column vector $c$ whose $i^{th}$ row entry is $v_i$. 

By the previous lemma, there is $\tilde{B}(t)$ such that $\tilde{B}(t) B(t)$ is $\det(t \cdot I_n - f)$ times the identity matrix. It follows that 

$$\det(t \cdot I_n - f) V = \tilde{B}(t) B(t) c = \tilde{B}(t) 0 = 0$$ 

i.e., $\det(t \cdot I_n - f) \cdot v_i = 0$ for each $i$. Since the $v_i$ form an $R$-basis, the $R[t]$-scalar $\det(t \cdot I_n - f)$ annihilates the $R[t]$-module $V$, as was to be shown. 
=-- 

The Cayley-Hamilton theorem easily generalizes to finitely generated $R$-modules (not necessarily free) as follows. Let $f \colon V \to V$ be a module endomorphism, and suppose $\pi \colon R^n \to V$ is an epimorphism. Since $R^n$ is projective, the map $f \circ \pi$ can be lifted through $\pi$ to a map $A \colon R^n \to R^n$. Let $P(t)$ be the characteristic polynomial of $A$. 

+-- {: .num_prop}
###### Proposition 
$P(f) = 0$. 
=-- 

+-- {: .proof} 
###### Proof 
Write $P(t) = \sum_i a_i t^i$. We already know $P(A) = 0$. From $f \circ \pi = \pi \circ A$, it follows that $f^i \circ \pi = \pi \circ A^i$ for any $i \geq 0$. Hence $P(f) \circ \pi = \pi \circ P(A) = 0$. Since $\pi$ is epic, $P(f) = 0$ follows. 
=-- 

We give an interesting and perhaps surprising consequence of the Cayley-Hamilton theorem below, after establishing a lemma close in spirit to [[Nakayama's lemma]]. 

+-- {: .num_lem} 
###### Lemma 
Suppose $V$ is a finitely generated $R$-module, and $g \colon V \to V$ is a module map such that $g(V) \subseteq I V$ for some ideal $I$ of $R$. Then there is a polynomial $p(t) = t^n + a_1 t^{n-1} + \ldots + a_n$, with all $a_i \in I$, such that $p(g) = 0$. 
=-- 

+-- {: .proof} 
###### Proof 
For some finite $n \geq 0$, we have a surjective map $R^n \to M$, and by hypothesis we have a surjective map $I^n \to im(g)$ in 

$$\array{
 & & & & I^n \\
 & & & & \downarrow \\
R^n & \to & M & \stackrel{g}{\to} & im(g)
}$$

By projectivity of $R^n$, we can lift the bottom composite to a map $R^n \to I^n$ making the diagram commute. Let $A$ be the $R$-module map $R^n \to I^n \hookrightarrow R^n$, regarded as a matrix. Then the characteristic polynomial of $A$ satisfies the conclusion, by the Cayley-Hamilton theorem. 
=-- 

+-- {: .num_prop}
{#surj} 
###### Proposition 
Let $V$ be a finitely generated module over a commutative ring $R$, and let $f \colon V \to V$ be a surjective module map. Then $f$ is an isomorphism. 
=-- 

+-- {: .proof} 
###### Proof 
Regard $V$ as a finitely generated $R[t]$-module via $\phi_f \colon R[t] \to Mod_R(V, V)$. Since $f$ is assumed surjective, we have $I V = V$ for the ideal $I = (t)$ of $R[t]$. Now take $g = 1_V$ as in the preceding lemma, a module map over the ring $R' = R[t]$. By the lemma, we see that 
$g^n + a_1 g^{n-1} + \ldots + a_n = 0$ where $a_i \in (t)$, in other words the $R[t]$-scalar 

$$(1 + a_1 + \ldots + a_n)1_V = 0$$ 

as an operator on $V$. Write $a_i = b_i(t) t$ for polynomials $b_i(t) \in R[t]$. Now we may rewrite the previous displayed equation as 

$$1_V(v) = -(\sum_i b_i(t)) t \cdot v$$ 

for all $v \in V$, which translates into saying that $1_V = -\sum_i b_i(f) f$, i.e., that $-\sum_i b_i(f)$ is a retraction of $f$. Since $f$ is epic, we now see $f$ is an isomorphism. 
=-- 


### Over the real numbers 

A useful intuition to have for determinants of [[real number|real]] matrices is that they measure _change of volume_. That is, an $n \times n$ matrix with real entries will map a standard unit cube in $\mathbb{R}^n$ to a parallelpiped in $\mathbb{R}^n$ (quashed to lie in a hyperplane if the matrix is singular), and the determinant is, up to sign, the volume of the parallelpiped. It is easy to convince oneself of this in the planar case by a simple dissection of a parallelogram, rearranging the dissected pieces in the style of Euclid to form a rectangle. In algebraic terms, the dissection and rearrangement amount to applying shearing or elementary column operations to the matrix which, by the properties discussed earlier, leave the determinant unchanged. These operations transform the matrix into a diagonal matrix whose determinant is the area of the corresponding rectangle. This procedure easily generalizes to $n$ dimensions. 

The sign itself is a matter of interest. An invertible transformation $f \colon V \to V$ is said to be **[[orientation]]-preserving** if $\det(f)$ is positive, and **orientation-reversing** if $\det(f)$ is negative. Orientations play an important role throughout geometry and algebraic topology, for example in the study of orientable manifolds (where the tangent bundle as $GL(n)$-bundle can be lifted to a $GL_+(n)$-bundle structure, $GL_+(n) \hookrightarrow GL(n)$ being the subgroup of matrices of positive determinant). See also [[KO-theory]]. 

Finally, we include one more property of determinants which pertains to matrices with real coefficients (which works slightly more generally for matrices with coefficients in a [[local field]]): 

* If $A$ is an $n \times n$ matrix, then $\det(\exp(A)) =$ $\exp($[[trace]]$(A))$ 


## In terms of Berezinian integrals

see [[Pfaffian]] for the moment

## Related entries

* [[Pfaffian]], [[determinant line]], [[determinant line bundle]], [[quasideterminant]], [[resultant]]

## References 

The proof of the Cayley-Hamilton theorem follows the treatment in 

* Serge Lange, _Algebra_ ($3^{rd}$ edition), Addison-Wesley, 1993. 

The proof of [Proposition 3](#surj) on surjective endomorphisms of finitely generated modules was extracted from 

* Stacks Project, Commutative Algebra, section 13 ([pdf](http://math.columbia.edu/algebraic_geometry/stacks-git/algebra.pdf))

[[!redirects determinants]]