

> This [[HomePage|nLab]] page is for developing preliminary notes or making typographical experiments, etc. It may be edited by anybody, anytime. But you don't necessarily need to delete other people's ongoing notes here in order to add your own. In any case, overwritten edits may always be recovered from the [page history](/nlab/history/Sandbox).

\linebreak


***





++++++++++++++++++++++++++++++++++++++++++++


*Rethinking Topological Quantum Logic*

I Problem

- quantum computers excessively more powerful than classical ones -- in principle

- possible dramatic enhancement for machine learning algorithms -- in principle

- but quantum is immensely noise intolerant $\Rightarrow$ existing quantum computers are puny

- popular hope: live with the noise and fight it by software: "quantum error correction"

- more profound approach: prevent noise at hardware level by fundamental physical effects!

- tantalizing candidate: *topological quantum effects* keep information in *knotted* quantum states
  

- but theoretical understanding remained superficial & experimental claims remain dubious

- because topological quantum is "non-perturbative" physics: a $1M "Millennium Problem"

II Solution

- recent progress @CQTS [LMP 115 36 (2025)] via strongly coupled "M-branes" ("M-Theory"):

- topological states in "fractional quantum Hall" systems carried by exotic magnetic flux quanta

- but state-of-the-art understanding of magnetic "flux quantization" was a century old (Dirac 1931)

- novel math developed @CQTS [EoMP 4 (2025) 281] explains generalized exotic flux quantization

- shows there exists an exotic "flux quantization law" which knows fractional Hall quantum topology!

- this novel law ("Hypothesis H") predicts pathway to topological quantum gates via "defect anyons"

- I [will have] just come from a meeting at ShabaniLab@NYU NY, discussing experimental prospects


=> gate opened towards scaling-up quantum-computing towards practical quantum machine learning?


Thanks for your attention.




