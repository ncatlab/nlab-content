
#Contents#
* automatic table of contents goes here
{:toc}


## Idea

Information geometry aims to apply the techniques of [[differential geometry]] to [[statistics]]. Often it is useful to think of a family of probability distributions as a _statistical_ [[manifold]]. For example, normal [[Gaussian distribution]]s form a 2-[[dimension]]al manifold, parameterised by $(\mu, \sigma)$, mean and standard deviation. On such manifolds there are notions of [[Riemannian metric]], [[connection]], [[curvature]], and so on, of statistical relevance.

More precisely, 

> [[Kullback-Leibler information]], or [[relative entropy]], features as a measure of [[divergence]] (not quite a [[metric]], because it's asymmetric), and [[Fisher information]] takes the role of [[curvature]]. One useful aspect of information geometry is that it gives a means to prove results about statistical models, simply by considering them as well-behaved geometrical objects. For instance, it's basically a tautology to say that a [[manifold]] is not changing much in the vicinity of points of low curvature, and changing greatly near points of high curvature. Stated more precisely, and then translated back into probabilistic language, this becomes the [[Cramer-Rao inequality]], that the variance of a parameter estimator is at least the reciprocal of the [[Fisher information]]. ([Shalizi](#Shalizi))

One of the founders of the subject is Shun-ichi [Amari](http://www.brain.riken.jp/labs/mns/amari/home-E.html).

## Definitions

For $X$ a [[measurable space]] let $S$ be (a subspace of) the space of probability [[measure]]s on $X$, equipped with the structure of a [[smooth manifold]].

The **Fisher metric** on $S$ is the [[Riemannian metric]] given on two [[vector field]]s $v,w \in T S$ by

$$
  g(v,w)_s := E_s( v(log s) w(log s))
  \,,
$$

where $E_s(\cdots)$ denotes the [[expectation value]] under the measure $s \in S$ of the function $x \mapsto  v(log s)_x w(log s)_x$ on $X$.

For instance ([Amari, (2.1)](#AmariTextbook)).

## References

A textbook providing the big picture is

* Shun-ichi Amari, O. E. Barndorff-Nielsen, R. E. Kass, S. L. Lauritzen, and C. R. Rao, _Differential geometry in statistical inference_ ([project Euclid](http://projecteuclid.org/euclid.lnms/1215467056))
 {#AmariTextbook}

*  Shun-ichi Amari, Hiroshi Nagaoka, _Methods of information geometry_, Transactions of mathematical monographs; v. 191, American Mathematical Society, 2000. 

For a series of articles, see

* [[John Baez]], [Information Geometry](http://math.ucr.edu/home/baez/information/)


Lecture notes include

*  Shun-ichi Amari, MaxEnt 2007 [lecture](http://video.google.com/videoplay?docid=2175767568645553282#)
*  Shun-ichi Amari, ETVC08 [lecture](http://videolectures.net/etvc08_amari_igaia/).
*  Sanjoy Dasgupta, [lecture](http://videolectures.net/mlss05us_dasgupta_ig/).

See also

*  H&#244;ng V&#226;n L&#234;, Statistical manifolds are statistical models, Journal of Geometry 84(1-2), March 2006, pp. 83-93.


*  Blog [post](http://golem.ph.utexas.edu/category/2006/11/infinitedimensional_exponentia.html).

A brief introduction with more references is

* Cosma Shalizi, _Information geometry_ ([webpage](http://www.cscs.umich.edu/~crshalizi/notabene/info-geo.html))
 {#Shalizi}
